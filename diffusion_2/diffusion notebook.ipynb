{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import UNet\n",
    "from diffusion_discrete import DiscreteDiffusion, generate_betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if gpu else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(50)\n",
    "\n",
    "# training parameters\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "lr = 2e-4\n",
    "model = UNet(image_channels=1).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters:  168897282\n",
      "UNet(\n",
      "  (image_proj): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (time_emb): TimeEmbedding(\n",
      "    (lin1): Linear(in_features=64, out_features=256, bias=True)\n",
      "    (act): Swish()\n",
      "    (lin2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (down): ModuleList(\n",
      "    (0-1): 2 x DownBlock(\n",
      "      (res): ResidualBlock(\n",
      "        (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "        (act1): Swish()\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "        (act2): Swish()\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (shortcut): Identity()\n",
      "        (time_emb): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (time_act): Swish()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attn): Identity()\n",
      "    )\n",
      "    (2): Downsample(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (3): DownBlock(\n",
      "      (res): ResidualBlock(\n",
      "        (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "        (act1): Swish()\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "        (act2): Swish()\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (shortcut): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (time_emb): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (time_act): Swish()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attn): Identity()\n",
      "    )\n",
      "    (4): DownBlock(\n",
      "      (res): ResidualBlock(\n",
      "        (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "        (act1): Swish()\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "        (act2): Swish()\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (shortcut): Identity()\n",
      "        (time_emb): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (time_act): Swish()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attn): Identity()\n",
      "    )\n",
      "    (5): Downsample(\n",
      "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (6): DownBlock(\n",
      "      (res): ResidualBlock(\n",
      "        (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "        (act1): Swish()\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        (act2): Swish()\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (time_emb): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (time_act): Swish()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attn): AttentionBlock(\n",
      "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        (projection): Linear(in_features=256, out_features=768, bias=True)\n",
      "        (output): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (7): DownBlock(\n",
      "      (res): ResidualBlock(\n",
      "        (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        (act1): Swish()\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        (act2): Swish()\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (shortcut): Identity()\n",
      "        (time_emb): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (time_act): Swish()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attn): AttentionBlock(\n",
      "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        (projection): Linear(in_features=256, out_features=768, bias=True)\n",
      "        (output): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (8): Downsample(\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (9): DownBlock(\n",
      "      (res): ResidualBlock(\n",
      "        (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        (act1): Swish()\n",
      "        (conv1): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
      "        (act2): Swish()\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (shortcut): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (time_emb): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (time_act): Swish()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attn): AttentionBlock(\n",
      "        (norm): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
      "        (projection): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (10): DownBlock(\n",
      "      (res): ResidualBlock(\n",
      "        (norm1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
      "        (act1): Swish()\n",
      "        (conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
      "        (act2): Swish()\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (shortcut): Identity()\n",
      "        (time_emb): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (time_act): Swish()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attn): AttentionBlock(\n",
      "        (norm): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
      "        (projection): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (middle): MiddleBlock(\n",
      "    (res1): ResidualBlock(\n",
      "      (norm1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
      "      (act1): Swish()\n",
      "      (conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (norm2): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
      "      (act2): Swish()\n",
      "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (shortcut): Identity()\n",
      "      (time_emb): Linear(in_features=256, out_features=1024, bias=True)\n",
      "      (time_act): Swish()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (attn): AttentionBlock(\n",
      "      (norm): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
      "      (projection): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "      (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (res2): ResidualBlock(\n",
      "      (norm1): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
      "      (act1): Swish()\n",
      "      (conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (norm2): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
      "      (act2): Swish()\n",
      "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (shortcut): Identity()\n",
      "      (time_emb): Linear(in_features=256, out_features=1024, bias=True)\n",
      "      (time_act): Swish()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (up): ModuleList(\n",
      "    (0-1): 2 x UpBlock(\n",
      "      (res): ResidualBlock(\n",
      "        (norm1): GroupNorm(32, 2048, eps=1e-05, affine=True)\n",
      "        (act1): Swish()\n",
      "        (conv1): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
      "        (act2): Swish()\n",
      "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (shortcut): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (time_emb): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (time_act): Swish()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attn): AttentionBlock(\n",
      "        (norm): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
      "        (projection): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (output): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): UpBlock(\n",
      "      (res): ResidualBlock(\n",
      "        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "        (act1): Swish()\n",
      "        (conv1): Conv2d(1280, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        (act2): Swish()\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (shortcut): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (time_emb): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (time_act): Swish()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attn): AttentionBlock(\n",
      "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        (projection): Linear(in_features=256, out_features=768, bias=True)\n",
      "        (output): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): Upsample(\n",
      "      (conv): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (4-5): 2 x UpBlock(\n",
      "      (res): ResidualBlock(\n",
      "        (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "        (act1): Swish()\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        (act2): Swish()\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (time_emb): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (time_act): Swish()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attn): AttentionBlock(\n",
      "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        (projection): Linear(in_features=256, out_features=768, bias=True)\n",
      "        (output): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (6): UpBlock(\n",
      "      (res): ResidualBlock(\n",
      "        (norm1): GroupNorm(32, 384, eps=1e-05, affine=True)\n",
      "        (act1): Swish()\n",
      "        (conv1): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "        (act2): Swish()\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (shortcut): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (time_emb): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (time_act): Swish()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attn): AttentionBlock(\n",
      "        (norm): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "        (projection): Linear(in_features=128, out_features=384, bias=True)\n",
      "        (output): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (7): Upsample(\n",
      "      (conv): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (8-9): 2 x UpBlock(\n",
      "      (res): ResidualBlock(\n",
      "        (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        (act1): Swish()\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "        (act2): Swish()\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (time_emb): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (time_act): Swish()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attn): Identity()\n",
      "    )\n",
      "    (10): UpBlock(\n",
      "      (res): ResidualBlock(\n",
      "        (norm1): GroupNorm(32, 192, eps=1e-05, affine=True)\n",
      "        (act1): Swish()\n",
      "        (conv1): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "        (act2): Swish()\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (shortcut): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (time_emb): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (time_act): Swish()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attn): Identity()\n",
      "    )\n",
      "    (11): Upsample(\n",
      "      (conv): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (12-14): 3 x UpBlock(\n",
      "      (res): ResidualBlock(\n",
      "        (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "        (act1): Swish()\n",
      "        (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "        (act2): Swish()\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (shortcut): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (time_emb): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (time_act): Swish()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attn): Identity()\n",
      "    )\n",
      "  )\n",
      "  (norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n",
      "  (act): Swish()\n",
      "  (final): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of model parameters: \", count_parameters(model))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change image size from 28 to 32 so that it is power of 2\n",
    "img_size = 32\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = generate_betas(type='cosine', start=0.02, stop=1., num_steps=1000).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/madsfrandsen/Documents/BSc_project/diffusion_2/diffusion_discrete.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.betas = betas = torch.tensor(betas, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "diffusion = DiscreteDiffusion(betas=betas, transition_mat_type='uniform',\n",
    "                              num_bits=8, transition_bands=None, model_prediction='x_start',\n",
    "                              model_output='logistic_pars', loss_type='hybrid',\n",
    "                              hybrid_coeff=0.001, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/469 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/madsfrandsen/Documents/BSc_project/diffusion_2/utils.py:89: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  log_probs = F.log_softmax(logits)\n",
      "  0%|          | 2/469 [03:37<14:07:26, 108.88s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mint()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 10\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mdiffusion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     11\u001b[0m prior_bpd \u001b[38;5;241m=\u001b[39m diffusion\u001b[38;5;241m.\u001b[39mprior_bpd(x)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Documents/BSc_project/diffusion_2/diffusion_discrete.py:516\u001b[0m, in \u001b[0;36mDiscreteDiffusion.training_losses\u001b[0;34m(self, model_fn, x_start, rng)\u001b[0m\n\u001b[1;32m    511\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_entropy_start(\n\u001b[1;32m    512\u001b[0m         x_start\u001b[38;5;241m=\u001b[39mx_start, pred_x_start_logits\u001b[38;5;241m=\u001b[39mpred_x_start_logits)\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhybrid\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;66;03m# Optimizes L_vb - lambda * sum_x_start x_start log pred_x_start.\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m     vb_losses, pred_x_start_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvb_terms_bpd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m     ce_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_entropy_start(\n\u001b[1;32m    519\u001b[0m         x_start\u001b[38;5;241m=\u001b[39mx_start, pred_x_start_logits\u001b[38;5;241m=\u001b[39mpred_x_start_logits)\n\u001b[1;32m    520\u001b[0m     losses \u001b[38;5;241m=\u001b[39m vb_losses \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhybrid_coeff \u001b[38;5;241m*\u001b[39m ce_losses\n",
      "File \u001b[0;32m~/Documents/BSc_project/diffusion_2/diffusion_discrete.py:418\u001b[0m, in \u001b[0;36mDiscreteDiffusion.vb_terms_bpd\u001b[0;34m(self, model_fn, x_start, x_t, t)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calculate specified terms of the variational bound.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \n\u001b[1;32m    405\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;124;03m    the denoised image.\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    417\u001b[0m true_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_posterior_logits(x_start, x_t, t, x_start_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 418\u001b[0m model_logits, pred_x_start_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m kl \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mcategorical_kl_logits(logits1\u001b[38;5;241m=\u001b[39mtrue_logits, logits2\u001b[38;5;241m=\u001b[39mmodel_logits)\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m kl\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m x_start\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/Documents/BSc_project/diffusion_2/diffusion_discrete.py:286\u001b[0m, in \u001b[0;36mDiscreteDiffusion.p_logits\u001b[0;34m(self, model_fn, x, t)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute logits of p(x_{t-1} | x_t).\"\"\"\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m t\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],)\n\u001b[0;32m--> 286\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_output \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    289\u001b[0m     model_logits \u001b[38;5;241m=\u001b[39m model_output\n",
      "File \u001b[0;32m~/Documents/BSc_project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/BSc_project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/BSc_project/diffusion_2/model.py:425\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# First half of U-Net\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown:\n\u001b[0;32m--> 425\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m     h\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# Middle (bottom)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/BSc_project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/BSc_project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/BSc_project/diffusion_2/model.py:230\u001b[0m, in \u001b[0;36mDownBlock.forward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, t: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 230\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mres\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(x)\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Documents/BSc_project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/BSc_project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/BSc_project/diffusion_2/model.py:136\u001b[0m, in \u001b[0;36mResidualBlock.forward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m* `x` has shape `[batch_size, in_channels, height, width]`\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m* `t` has shape `[batch_size, time_channels]`\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# First convolution layer\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Add time embeddings\u001b[39;00m\n\u001b[1;32m    138\u001b[0m h \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_emb(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_act(t))[:, :, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "File \u001b[0;32m~/Documents/BSc_project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/BSc_project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for e in range(1, num_epochs+1):\n",
    "    train_loss = 0\n",
    "    train_loss_vals = []\n",
    "    train_prior_bpd = 0\n",
    "    for batch_idx, (x, _) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        x = x.int().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = diffusion.training_losses(model, x_start=x, rng=25).mean()\n",
    "        prior_bpd = diffusion.prior_bpd(x).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        train_prior_bpd += prior_bpd\n",
    "        train_loss_vals.append(train_loss)\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "    print(\"t\\Epoch,\", e, \"complete!\", \"\\tLoss: \", train_loss / batch_idx,\n",
    "          \"\\tPrior bpd: \", train_prior_bpd / batch_idx)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
