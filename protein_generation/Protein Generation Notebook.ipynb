{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from model import ByteNetLMTime\n",
    "from dataset import ProteinSequenceDataset, Collater\n",
    "from losses import D3PMLVBLoss\n",
    "from utils import Tokenizer\n",
    "from constants import PROTEIN_ALPHABET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if gpu else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed for reproducability\n",
    "torch.manual_seed(50)\n",
    "\n",
    "# model parameters\n",
    "n_tokens = len(PROTEIN_ALPHABET)\n",
    "d_embed = 8\n",
    "d_model = 1024\n",
    "activation = 'gelu'\n",
    "slim = True\n",
    "n_layers = 16\n",
    "kernel_size = 5\n",
    "r = 128\n",
    "diffusion_timesteps = 500\n",
    "\n",
    "# training parameters\n",
    "num_epochs = 500\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "model = ByteNetLMTime(n_tokens, d_embed, d_model, n_layers, kernel_size, r,\n",
    "                    activation=activation, slim=slim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "_lambda = 0\n",
    "\n",
    "# datasets\n",
    "data_train = ProteinSequenceDataset(train=True)\n",
    "data_test = ProteinSequenceDataset(train=False)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "Q_prod, Q_t = tokenizer.q_uniform_schedule(num_steps=diffusion_timesteps)\n",
    "collater = Collater(tokenizer=tokenizer, num_steps=diffusion_timesteps, Q=Q_t, Q_bar=Q_prod)\n",
    "train_loader = DataLoader(dataset=data_train, batch_size=batch_size, collate_fn=collater)\n",
    "\n",
    "# loss function\n",
    "loss_func1 = D3PMLVBLoss(tmax=diffusion_timesteps, tokenizer=tokenizer)\n",
    "loss_func2 = torch.nn.CrossEntropyLoss(weight=None, reduction='mean')\n",
    "\n",
    "scaler = GradScaler() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters:  37878982\n",
      "ByteNetLMTime(\n",
      "  (embedder): ByteNetTime(\n",
      "    (time_encoding): PositionalEncoding1D()\n",
      "    (embedder): Embedding(22, 8)\n",
      "    (up_embedder): PositionFeedForward(\n",
      "      (conv): Conv1d(8, 1024, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): ByteNetBlock(\n",
      "        (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (sequence1): Sequential(\n",
      "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "          (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (4): GELU(approximate='none')\n",
      "        )\n",
      "        (sequence2): Sequential(\n",
      "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): ByteNetBlock(\n",
      "        (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "        (sequence1): Sequential(\n",
      "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "          (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (4): GELU(approximate='none')\n",
      "        )\n",
      "        (sequence2): Sequential(\n",
      "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): ByteNetBlock(\n",
      "        (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "        (sequence1): Sequential(\n",
      "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "          (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (4): GELU(approximate='none')\n",
      "        )\n",
      "        (sequence2): Sequential(\n",
      "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): ByteNetBlock(\n",
      "        (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "        (sequence1): Sequential(\n",
      "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "          (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (4): GELU(approximate='none')\n",
      "        )\n",
      "        (sequence2): Sequential(\n",
      "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): ByteNetBlock(\n",
      "        (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(32,), dilation=(16,))\n",
      "        (sequence1): Sequential(\n",
      "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "          (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (4): GELU(approximate='none')\n",
      "        )\n",
      "        (sequence2): Sequential(\n",
      "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): ByteNetBlock(\n",
      "        (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(64,), dilation=(32,))\n",
      "        (sequence1): Sequential(\n",
      "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "          (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (4): GELU(approximate='none')\n",
      "        )\n",
      "        (sequence2): Sequential(\n",
      "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): ByteNetBlock(\n",
      "        (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(128,), dilation=(64,))\n",
      "        (sequence1): Sequential(\n",
      "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "          (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (4): GELU(approximate='none')\n",
      "        )\n",
      "        (sequence2): Sequential(\n",
      "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): ByteNetBlock(\n",
      "        (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(256,), dilation=(128,))\n",
      "        (sequence1): Sequential(\n",
      "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "          (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (4): GELU(approximate='none')\n",
      "        )\n",
      "        (sequence2): Sequential(\n",
      "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): ByteNetBlock(\n",
      "        (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "        (sequence1): Sequential(\n",
      "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "          (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (4): GELU(approximate='none')\n",
      "        )\n",
      "        (sequence2): Sequential(\n",
      "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): ByteNetBlock(\n",
      "        (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(4,), dilation=(2,))\n",
      "        (sequence1): Sequential(\n",
      "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "          (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (4): GELU(approximate='none')\n",
      "        )\n",
      "        (sequence2): Sequential(\n",
      "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): ByteNetBlock(\n",
      "        (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(8,), dilation=(4,))\n",
      "        (sequence1): Sequential(\n",
      "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "          (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (4): GELU(approximate='none')\n",
      "        )\n",
      "        (sequence2): Sequential(\n",
      "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): ByteNetBlock(\n",
      "        (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(16,), dilation=(8,))\n",
      "        (sequence1): Sequential(\n",
      "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "          (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (4): GELU(approximate='none')\n",
      "        )\n",
      "        (sequence2): Sequential(\n",
      "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (12): ByteNetBlock(\n",
      "        (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(32,), dilation=(16,))\n",
      "        (sequence1): Sequential(\n",
      "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "          (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (4): GELU(approximate='none')\n",
      "        )\n",
      "        (sequence2): Sequential(\n",
      "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): ByteNetBlock(\n",
      "        (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(64,), dilation=(32,))\n",
      "        (sequence1): Sequential(\n",
      "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "          (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (4): GELU(approximate='none')\n",
      "        )\n",
      "        (sequence2): Sequential(\n",
      "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): ByteNetBlock(\n",
      "        (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(128,), dilation=(64,))\n",
      "        (sequence1): Sequential(\n",
      "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "          (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (4): GELU(approximate='none')\n",
      "        )\n",
      "        (sequence2): Sequential(\n",
      "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (15): ByteNetBlock(\n",
      "        (conv): MaskedConv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(256,), dilation=(128,))\n",
      "        (sequence1): Sequential(\n",
      "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "          (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (4): GELU(approximate='none')\n",
      "        )\n",
      "        (sequence2): Sequential(\n",
      "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): PositionFeedForward(\n",
      "            (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): PositionFeedForward(\n",
      "    (conv): Conv1d(1024, 22, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (last_norm): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of model parameters: \", count_parameters(model))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/93 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33664, 22])\n",
      "torch.Size([33664])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/93 [03:44<5:43:36, 224.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33664, 22])\n",
      "torch.Size([33664])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/93 [08:16<6:23:09, 252.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33664, 22])\n",
      "torch.Size([33664])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/93 [13:37<6:48:42, 272.47s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(enabled\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available(), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32):\n\u001b[0;32m---> 28\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, outputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tgt\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/Documents/BSc_project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/BSc_project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/BSc_project/protein_generation/model.py:168\u001b[0m, in \u001b[0;36mByteNetLMTime.forward\u001b[0;34m(self, x, y, input_mask)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y, input_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 168\u001b[0m     e \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m     e \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_norm(e)\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(e)\n",
      "File \u001b[0;32m~/Documents/BSc_project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/BSc_project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/BSc_project/protein_generation/model.py:126\u001b[0m, in \u001b[0;36mByteNetTime.forward\u001b[0;34m(self, x, y, input_mask)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m:param x: (batch, length)\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m:param y: (batch)\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m:param input_mask: (batch, length, 1)\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m:return: (batch, length,)\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m e \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed(x, y, timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimesteps)\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/BSc_project/protein_generation/model.py:141\u001b[0m, in \u001b[0;36mByteNetTime._convolve\u001b[0;34m(self, e, input_mask)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convolve\u001b[39m(\u001b[38;5;28mself\u001b[39m, e, input_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 141\u001b[0m         e \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    143\u001b[0m             e \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(e, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout)\n",
      "File \u001b[0;32m~/Documents/BSc_project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/BSc_project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/BSc_project/venv/lib/python3.10/site-packages/sequence_models/convolutional.py:249\u001b[0m, in \u001b[0;36mByteNetBlock.forward\u001b[0;34m(self, x, input_mask)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, input_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    243\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m    :param x: (batch, length, in_channels)\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m    :param input_mask: (batch, length, 1)\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;124;03m    :return: (batch, length, out_channels)\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence2(\n\u001b[0;32m--> 249\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequence1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, input_mask\u001b[38;5;241m=\u001b[39minput_mask)\n\u001b[1;32m    250\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/BSc_project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/BSc_project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/BSc_project/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/BSc_project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/BSc_project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/BSc_project/venv/lib/python3.10/site-packages/torch/nn/modules/activation.py:681\u001b[0m, in \u001b[0;36mGELU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapproximate \u001b[38;5;241m=\u001b[39m approximate\n\u001b[0;32m--> 681\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mgelu(\u001b[38;5;28minput\u001b[39m, approximate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapproximate)\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextra_repr\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for e in range(1, num_epochs+1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_loss_vals = []\n",
    "    train_prior_bpd = 0\n",
    "    for batch_idx, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "\n",
    "        src, src_onehot, timesteps, tgt, tgt_onehot, Q, Q_bar, q, names = batch\n",
    "\n",
    "        # move to data to device\n",
    "        q = q.to(device)\n",
    "        Q = Q.to(device)\n",
    "        Q_bar = Q_bar.to(device)\n",
    "        src_onehot = src_onehot.to(device)\n",
    "        tgt_onehot = tgt_onehot.to(device)\n",
    "        timesteps = timesteps.to(device)\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        # compute number of tokens in batch\n",
    "        n_tokens = src.shape[0] * src.shape[1]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast(enabled=torch.cuda.is_available(), dtype=torch.float32):\n",
    "            outputs = model(src, timesteps)\n",
    "            lvb_loss = loss_func1(src_onehot, q, outputs, tgt, tgt_onehot, timesteps, Q, Q_bar)\n",
    "            ce_loss = loss_func2(outputs.reshape(-1, outputs.shape[-1]), tgt.reshape(-1))\n",
    "            lvb_loss = lvb_loss.to(torch.float32)\n",
    "            ce_loss = ce_loss.to(torch.float32)\n",
    "            loss = (lvb_loss + (_lambda * ce_loss)) * n_tokens\n",
    "            nll_loss = ce_loss * n_tokens\n",
    "        \n",
    "        if scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_loss_vals.append(loss.item())\n",
    "    \n",
    "    train_loss /= batch_idx\n",
    "    train_losses.append(train_loss_vals)\n",
    "    \n",
    "    print(\"\\tEpoch,\", e, \"complete!\", \"\\tTrain Loss: \", train_loss)\n",
    "    if e % 10 == 0:\n",
    "        plt.plot(np.arange(1, e+1), [np.mean(ls) for ls in train_losses], lw=2.5, label='train')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        # plt.yscale('log')\n",
    "        plt.title('Average training loss per epoch')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    #     x = x.to(device, dtype=torch.int32)\n",
    "    #     # frame work expects data shape (B, H, W, C)\n",
    "    #     x = x.permute(0, 2, 3, 1)\n",
    "\n",
    "    #     optimizer.zero_grad()\n",
    "    #     loss = diffusion.training_losses(model, x_start=x, rng=25).mean()\n",
    "    #     prior_bpd = diffusion.prior_bpd(x).mean()\n",
    "\n",
    "    #     loss.backward()\n",
    "    #     train_loss += loss.item()\n",
    "    #     train_loss_vals.append(loss.item())\n",
    "    #     optimizer.step()\n",
    "\n",
    "    # train_loss /= batch_idx\n",
    "\n",
    "    # # evaluation\n",
    "    # model.eval()\n",
    "    # test_loss = 0\n",
    "    # test_loss_vals = []\n",
    "    # test_prior_bpd = 0\n",
    "    # test_total_bpd = 0\n",
    "    # with torch.no_grad():\n",
    "    #     for batch_idx, (x_test, _) in enumerate(test_loader):\n",
    "    #         x_test = x_test.to(device, dtype=torch.int32)\n",
    "    #         x_test = x_test.permute(0, 2, 3, 1)\n",
    "\n",
    "    #         l = diffusion.training_losses(model, x_start=x_test, rng=25).mean()\n",
    "    #         test_loss += l.item()\n",
    "    #         test_loss_vals.append(l.item())\n",
    "\n",
    "    #         # loss_dict = diffusion.calc_bpd_loop(model, x_start=x_test, rng=25)\n",
    "    #         # total_bpd = torch.mean(loss_dict['total'], dim=0)\n",
    "    #         # prior_bpd = torch.mean(loss_dict['prior'], axis=0)\n",
    "    \n",
    "    # test_loss /= batch_idx\n",
    "\n",
    "    # train_losses.append(train_loss_vals)\n",
    "    # test_losses.append(test_loss_vals)\n",
    "\n",
    "    # samples = diffusion.p_sample_loop(model_fn=model, shape=(1, 32, 32, 1), rng=25)\n",
    "    # imshow(samples[0].detach().cpu())\n",
    "\n",
    "    # print(\"\\tEpoch,\", e, \"complete!\", \"\\tTrain Loss: \", train_loss,\n",
    "    #       \"\\tTest Loss: \", test_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
